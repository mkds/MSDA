{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "### Objective\n",
    "Try to predict whether a movie review is positive or negative. \n",
    "For this text classification, bag of words technique is used to extract features. Two classifiers naive bayes and support vector machines are trained and used for predictions  \n",
    "\n",
    "### Data\n",
    "\n",
    "The dataset used for this text analysis is obtained from https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "The dataset contains 1000 postive and 1000 negative movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import required pacakges\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file  \n",
    "Each review is stored as one text file with in a zip file. We shall read all the files and store each file content as an element in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read review from zip file\n",
    "pos_zip_file= zipfile.ZipFile(\"DATA/pos1.zip\")\n",
    "#Each file has a review, so read each file as a list element\n",
    "pos_review=[pos_zip_file.open(f).read() for f in pos_zip_file.namelist()]\n",
    "label=[\"Positive\"]*len(pos_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_zip_file= zipfile.ZipFile(\"DATA/neg1.zip\")\n",
    "neg_review=[neg_zip_file.open(f).read() for f in neg_zip_file.namelist()]\n",
    "label=[\"Negative\"]*len(neg_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the negative and positive reviews and create labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine positive and negative reviews\n",
    "reviews = pos_review + neg_review\n",
    "#Labels for review\n",
    "label=[\"Positive\"]*len(pos_review) + [\"Negative\"]*len(neg_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize text\n",
    "Some of the language constructs used in natural language are not useful in predicting label. We need to normalize text in order to get features we are interested in. \n",
    "\n",
    "Following normalization techniques are used to normalize the movie reviews\n",
    "- Convert text to lower case\n",
    "- Replace special characters with space\n",
    "- Remove stop words\n",
    "- Stem the words\n",
    "- Remove words that are less than two characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This is a sample text with some words\n",
      "Normalized text: thi sampl text word\n"
     ]
    }
   ],
   "source": [
    "#Function to Normalize tex\n",
    "def normalize_text(comment):\n",
    "    #Convert to lowercase\n",
    "    comment=comment.lower()\n",
    "    #Remove special characters\n",
    "    comment=re.sub(\"[^a-zA-Z]\", \" \",comment)\n",
    "    #Get words\n",
    "    words = nltk.word_tokenize(comment)\n",
    "    #Stem words\n",
    "    porter = nltk.PorterStemmer()\n",
    "    words=[porter.stem(w) for w in words]\n",
    "    #Keep only words that are 3 or more character long\n",
    "    words=[w for w in words if len(w) > 2]\n",
    "    #Remove stop words\n",
    "    words=[w for w in words if w not in nltk.corpus.stopwords.words('english')]\n",
    "    return(\" \".join(words))\n",
    "    \n",
    "print \"Text: This is a sample text with some words\"\n",
    "print \"Normalized text:\",normalize_text(\"This is a sample text with some words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalize all reviews\n",
    "norm_reviews=[normalize_text(s) for s in reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction - Bag of words Technique\n",
    "In order to predict if a review is positive are negative we need extract features from review text and use it for prediction. We shall use bag of words technique to extract the features. We count the frequencies of the words in a review and use that for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words array: (2000, 500)\n"
     ]
    }
   ],
   "source": [
    "#Create bag of words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 500) #Use 500 most frequent words\n",
    "bag_of_words = vectorizer.fit_transform(norm_reviews).toarray()\n",
    "print \"Bag of words array:\",bag_of_words.shape\n",
    "#Get word feature\n",
    "feat_names=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into test and training set\n",
    "We shall use first 800 positive and first 800 negative reviews for training and last 200 positive and last 200 negative reviews for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=np.vstack((bag_of_words[800:1000,],bag_of_words[1800:,]))\n",
    "test_label=label[800:1000]+label[1800:]\n",
    "train=np.vstack((bag_of_words[:800,], bag_of_words[1000:1800,]))\n",
    "train_label=label[:800]+label[1000:1800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### NLTK Naive Bayes\n",
    "Let's try NLTK Navie Bayes to classify reviews. As NLTK Navie Bayes expects nominal values in a list of dictionary we shall convert the word frequency to word presence (i.e. logical indicating if the word is present in the review or not) and create list of dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.808125\n",
      "Test Accuracy: 0.775\n"
     ]
    }
   ],
   "source": [
    "#Covert to word presence dictionary\n",
    "train_dict=[(dict(zip(feat_names,row)),train_label[index]) \n",
    "                   for index,row in enumerate(train.astype(bool))]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_dict)\n",
    "\n",
    "test_dict=[(dict(zip(feat_names,row)),test_label[index]) for index,row in enumerate(test.astype(bool))]\n",
    "print \"Training Accuracy:\",nltk.classify.accuracy(classifier,train_dict)\n",
    "print \"Test Accuracy:\",nltk.classify.accuracy(classifier,test_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words that indicate the Review type\n",
    "We could check the informative features of classifier to check the words that indicate positive or negative reviews. As we could see from the list below the words \"wast\" (i.e waste), worst, stupid etc indicate negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    wast = True           Negati : Positi =      4.7 : 1.0\n",
      "                   worst = True           Negati : Positi =      4.3 : 1.0\n",
      "                  stupid = True           Negati : Positi =      4.3 : 1.0\n",
      "                 portray = True           Positi : Negati =      2.9 : 1.0\n",
      "                    bore = True           Negati : Positi =      2.9 : 1.0\n",
      "                   oscar = True           Positi : Negati =      2.4 : 1.0\n",
      "                  suppos = True           Negati : Positi =      2.2 : 1.0\n",
      "                 sometim = True           Positi : Negati =      2.2 : 1.0\n",
      "                 perfect = True           Positi : Negati =      2.2 : 1.0\n",
      "                   touch = True           Positi : Negati =      2.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### SVM Classifier\n",
    "Let's try support vector machine (svm) classifer as SVM is good for linearly separable cases and the text classifications are often linearly separable. For SVM classifiers we shall use word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.938125\n",
      "Testing Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "svm_classifer=svm.SVC().fit(train,train_label)    \n",
    "print \"Training Accuracy:\",svm_classifer.score(train,train_label)\n",
    "print \"Testing Accuracy:\",svm_classifer.score(test,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM classifer using word frequencies has a high training accuracy and the testing accuracy is also better than our previous classifier which only used the presence of word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of adding more features\n",
    "For earlier models we used 500 most frequent words. Let's try the models with more features, say 1000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words array: (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Create bag of words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 1000) #Use 1000 most frequent words\n",
    "bag_of_words = vectorizer.fit_transform(norm_reviews).toarray()\n",
    "print \"Bag of words array:\",bag_of_words.shape\n",
    "#Get word feature\n",
    "feat_names=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.824375\n",
      "Test Accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "#Test and Train Split\n",
    "test=np.vstack((bag_of_words[800:1000,],bag_of_words[1800:,]))\n",
    "test_label=label[800:1000]+label[1800:]\n",
    "train=np.vstack((bag_of_words[:800,], bag_of_words[1000:1800,]))\n",
    "train_label=label[:800]+label[1000:1800]\n",
    "\n",
    "#NLTK NB\n",
    "#Covert to word presence dictionary\n",
    "train_dict=[(dict(zip(feat_names,row)),train_label[index]) \n",
    "                   for index,row in enumerate(train.astype(bool))]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_dict)\n",
    "\n",
    "test_dict=[(dict(zip(feat_names,row)),test_label[index]) for index,row in enumerate(test.astype(bool))]\n",
    "print \"Training Accuracy:\",nltk.classify.accuracy(classifier,train_dict)\n",
    "print \"Test Accuracy:\",nltk.classify.accuracy(classifier,test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and testing accuracies have improved, but only marginally for navie bayes classifer. Let's check the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.935625\n",
      "Testing Accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "svm_classifer=svm.SVC().fit(train,train_label)    \n",
    "print \"Training Accuracy:\",svm_classifer.score(train,train_label)\n",
    "print \"Testing Accuracy:\",svm_classifer.score(test,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM classifier the traning accuracy slightly dropped while the testing accuracy slightly improved.\n",
    "\n",
    "### Summary\n",
    "For the given dataset with movie review, we could predict if a review is postivie or negative with 80% accuracy using bag of words technqiue. Both the naive bayes and support vector machine performed well and the support vector machine performed slightly better than the Naive Bayes for this dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
