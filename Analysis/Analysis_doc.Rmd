---
title: "Analysis"
output:
  word_document: null
  pdf_document: default
  html_document:
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
library(knitr)
```

```{r}
#Read Data
suppressMessages(library(data.table))
#Read File
news = fread("OnlineNewsPopularity.csv", stringsAsFactors=F)
dim(news)
#Extract publication date from URL
news[,pub_date:= as.Date(gsub(".*mashable.com/(.*/.*/).*","\\1",url))]
```



```{r}
#Question 1
#Unique URL
print("Unique number of URLs")
print(length(unique(news$url)), row.names=F)
#As there is no duplicates URL all the records are unique 
#Find the largest lag between publication and data collection 
cat("\nLargest lag (in days) between publication and data collection")
print(max(news$timedelta), row.names=F)
#Time Frame of publication
summary(news$pub_date)
```

```{r}
#Question 2
#Histogram of timedelta
hist(news$timedelta,breaks = 73,
     main="Days between publication and data collection",
     xlab="Number of days", col="orchid")
suppressMessages(library(ggplot2))

#Timedelta by month
timedelta_bymonth = news[,.(mean_timedelta=mean(timedelta)),
                         by=(month=format(pub_date,"%Y-%m"))]
ggplot(timedelta_bymonth) + 
  geom_bar(aes(x=month,y=mean_timedelta),stat="identity",fill="orchid") +
  labs(x="Year-Month",y="Days",
       title="Mean lag between publication and data collection") +
  theme(axis.text.x = element_text(angle = 70, hjust = 1))

```

```{r}
#Question 3
#Extract topic from URL
topics <- gsub(".*/(.*)/$","\\1",news$url)
#Get topic Frequency
topics_freq <- as.data.frame(table(topics))
#Topics are unique, hence maximum frequency is 1
top20 <- topics_freq[order(topics_freq$Freq,decreasing = T)[1:20],]
kable(top20, caption="**Topic Frequency**") #Topics are unique, hence maximum frequency is 1

# **Try topic frequency by first word and last word**
# Get firstword
topic_firstword <- gsub("-.*","",topics)
# Get lastword
topic_lastword <- gsub(".*-","",topics)
#Frequency by first word
firstword_freq <- as.data.frame(table(topic_firstword))
#Frequency by last word
lastword_freq <- as.data.frame(table(topic_lastword))
#Get top 25 
top25_firstword <- firstword_freq[order(firstword_freq$Freq,
                                        decreasing = T)[1:25],]
top25_lastword <- lastword_freq[order(lastword_freq$Freq,
                                      decreasing = T)[1:25],]
kable(top25_firstword,caption="Top 25 First word of topic")
kable(top25_lastword,caption="Top 25 Last word of topic" )

```

```{r}
#Question 4
subtopics <- c("elon-musk","facebook","ebola","ipad","iphone",
               "tornado","sharknado","taylor-swift")
for (s in subtopics){
  #Add a column using subtopic name and set its initial value to False
  news[,s:=F,with=F]
  #Find URLs that has the subtopic as substring
  sub_present <- grep(s,news$url)
  #Set the subtopic column to true if it is substring of url
  news[sub_present,s:=T,with=F]
}
#Count by subtopics
kable(news[,lapply(.SD,sum),.SDcols=subtopics],
      caption="Count by sub-topics (substring in url)")
```


```{r}
#Question 5
#Count of subtopics by month
sum_by_subtop_month <- news[,lapply(.SD,sum),
                            .SDcols=subtopics,
                            by=(Year_Month=format(pub_date,"%Y-%m"))]
kable(sum_by_subtop_month,caption="Count of sub-topics by Month")
```

```{r}
#
cols_to_analyze <- c("shares","num_videos","num_imgs",
                     "abs_title_subjectivity","abs_title_sentiment_polarity")
kable(news[,lapply(.SD,mean),.SDcols=cols_to_analyze,by=is_weekend],
      caption="Mean values Weekend Vs Non-Weekend")
```

```{r}
#Number of Shares Weekend Vs non-weekend for Entertainment News
entertain_shares <- news[data_channel_is_entertainment > 0,lapply(.SD,mean),
                        .SDcols=cols_to_analyze,
                        by=is_weekend]
#Number of Shares Weekend Vs non-weekend for lifestyle News
lifestyle_shares <- news[data_channel_is_lifestyle > 0,lapply(.SD,mean),
                         .SDcols=cols_to_analyze,
                         by=is_weekend]
#Number of Shares Weekend Vs non-weekend for Tech News
tech_shares <- news[data_channel_is_tech > 0,lapply(.SD,mean),
                    .SDcols=cols_to_analyze,
                    by=is_weekend]
#Number of Shares Weekend Vs non-weekend for World News
world_shares <- news[data_channel_is_world > 0,
                     lapply(.SD,mean),
                     .SDcols=cols_to_analyze,by=is_weekend]
kable(entertain_shares, 
      caption="Mean values Weekend Vs Non-Weekend for Entertainment News")
kable(lifestyle_shares,
      caption="Mean values Weekend Vs Non-Weekend for lifestyle News")
kable(tech_shares, 
      caption="Mean values Weekend Vs Non-Weekend for Tech News")
kable(world_shares, 
      caption="Mean values Weekend Vs Non-Weekend for World News")
```